"""
Handles data insertion into the database from CSV files.
This script populates the database with data from the CSV files generated by table_creator.py.
"""
import mysql.connector
import pandas as pd
import numpy as np
import os
import sys

# Add parent directory to path to import config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import config


def populate_table_from_csv(table_name, csv_file_path, cursor, use_insert_ignore=False):
    """
    Generic function to populate a table from a CSV file.
    
    Args:
        table_name: Name of the target table
        csv_file_path: Path to the CSV file (relative to script directory or absolute)
        cursor: Database cursor
        use_insert_ignore: If True, use INSERT IGNORE to skip duplicates
    """
    # Get the script directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # If path is relative, make it relative to script directory
    if not os.path.isabs(csv_file_path):
        full_path = os.path.join(script_dir, csv_file_path)
    else:
        full_path = csv_file_path
    
    # Read the CSV file
    df = pd.read_csv(full_path, low_memory=False)
    
    # Drop fully empty rows
    df.dropna(how='all', inplace=True)
    
    # Convert NaN values to None (MySQL compatible NULL)
    df = df.replace({np.nan: None})
    
    # Handle date columns
    if 'release_date' in df.columns:
        df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce').dt.date
        df['release_date'] = df['release_date'].replace({pd.NaT: None})
    
    # Prepare data tuples
    tuples = [tuple(row) for row in df.itertuples(index=False, name=None)]
    
    # Prepare SQL query
    columns = ", ".join(df.columns)
    placeholders = ", ".join(["%s"] * len(df.columns))
    
    if use_insert_ignore:
        sql_query = f"INSERT IGNORE INTO {table_name} ({columns}) VALUES ({placeholders})"
    else:
        sql_query = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"
    
    # Execute batch insert in chunks to avoid packet size limits
    if tuples:
        total_rows = len(tuples)
        batch_size = 1000
        inserted = 0
        for i in range(0, total_rows, batch_size):
            batch = tuples[i:i + batch_size]
            cursor.executemany(sql_query, batch)
            inserted += len(batch)
            if total_rows > batch_size:
                print(f"  Inserted {inserted}/{total_rows} rows into {table_name}...", end='\r')
        if total_rows > batch_size:
            print()  # New line after progress indicator
        print(f"Data inserted successfully into {table_name}! ({total_rows} rows)")
    else:
        print(f"No data to insert into {table_name}")


def populate_movie_cast_table(csv_file_path, cursor, batch_size=1000):
    """
    Populate movie_cast table. Handle composite primary key.
    
    Args:
        csv_file_path: Path to the CSV file (relative to script directory or absolute)
        cursor: Database cursor
        batch_size: Number of rows to insert per batch (default: 1000)
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    if not os.path.isabs(csv_file_path):
        full_path = os.path.join(script_dir, csv_file_path)
    else:
        full_path = csv_file_path
        
    df = pd.read_csv(full_path, low_memory=False)
    
    # Drop fully empty rows
    df.dropna(how='all', inplace=True)
    
    # Convert NaN values to None (MySQL compatible NULL)
    df = df.replace({np.nan: None})
    
    # Remove duplicates based on all columns (handles composite key)
    df = df.drop_duplicates(subset=['movie_id', 'person_id', 'cast_order'])
    
    tuples = [tuple(row) for row in df.itertuples(index=False, name=None)]
    
    columns = ", ".join(df.columns)
    placeholders = ", ".join(["%s"] * len(df.columns))
    sql_query = f"INSERT INTO movie_cast ({columns}) VALUES ({placeholders})"
    
    if tuples:
        total_rows = len(tuples)
        inserted = 0
        for i in range(0, total_rows, batch_size):
            batch = tuples[i:i + batch_size]
            cursor.executemany(sql_query, batch)
            inserted += len(batch)
            if total_rows > batch_size:
                print(f"  Inserted {inserted}/{total_rows} rows into movie_cast...", end='\r')
        if total_rows > batch_size:
            print()  # New line after progress indicator
        print(f"Data inserted successfully into movie_cast! ({total_rows} rows)")
    else:
        print("No data to insert into movie_cast")


def populate_movie_crew_table(csv_file_path, cursor, batch_size=1000):
    """
    Populate movie_crew table. Handle composite primary key.
    
    Args:
        csv_file_path: Path to the CSV file (relative to script directory or absolute)
        cursor: Database cursor
        batch_size: Number of rows to insert per batch (default: 1000)
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    if not os.path.isabs(csv_file_path):
        full_path = os.path.join(script_dir, csv_file_path)
    else:
        full_path = csv_file_path
        
    df = pd.read_csv(full_path, low_memory=False)
    
    # Drop fully empty rows
    df.dropna(how='all', inplace=True)
    
    # Convert NaN values to None (MySQL compatible NULL)
    df = df.replace({np.nan: None})
    
    # Remove duplicates based on all columns (handles composite key)
    df = df.drop_duplicates(subset=['movie_id', 'person_id', 'department', 'job'])
    
    tuples = [tuple(row) for row in df.itertuples(index=False, name=None)]
    
    columns = ", ".join(df.columns)
    placeholders = ", ".join(["%s"] * len(df.columns))
    sql_query = f"INSERT INTO movie_crew ({columns}) VALUES ({placeholders})"
    
    if tuples:
        total_rows = len(tuples)
        inserted = 0
        for i in range(0, total_rows, batch_size):
            batch = tuples[i:i + batch_size]
            cursor.executemany(sql_query, batch)
            inserted += len(batch)
            if total_rows > batch_size:
                print(f"  Inserted {inserted}/{total_rows} rows into movie_crew...", end='\r')
        if total_rows > batch_size:
            print()  # New line after progress indicator
        print(f"Data inserted successfully into movie_crew! ({total_rows} rows)")
    else:
        print("No data to insert into movie_crew")


def populate_movie_ratings_summary(csv_file_path, cursor, batch_size=1000):
    """
    Populate movie_ratings_summary table, filtering to only include movie_ids that exist in movies table.
    
    Args:
        csv_file_path: Path to the CSV file (relative to script directory or absolute)
        cursor: Database cursor
        batch_size: Number of rows to insert per batch (default: 1000)
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    if not os.path.isabs(csv_file_path):
        full_path = os.path.join(script_dir, csv_file_path)
    else:
        full_path = csv_file_path
        
    df = pd.read_csv(full_path, low_memory=False)
    
    # Drop fully empty rows
    df.dropna(how='all', inplace=True)
    
    # Get list of valid movie_ids from the movies table
    cursor.execute("SELECT movie_id FROM movies")
    valid_movie_ids = set(row[0] for row in cursor.fetchall())
    
    # Filter ratings to only include movies that exist
    original_count = len(df)
    df = df[df['movie_id'].isin(valid_movie_ids)]
    filtered_count = len(df)
    
    if original_count > filtered_count:
        print(f"  Filtered {original_count - filtered_count} ratings for non-existent movies")
    
    # Convert NaN values to None (MySQL compatible NULL)
    df = df.replace({np.nan: None})
    
    tuples = [tuple(row) for row in df.itertuples(index=False, name=None)]
    
    columns = ", ".join(df.columns)
    placeholders = ", ".join(["%s"] * len(df.columns))
    sql_query = f"INSERT INTO movie_ratings_summary ({columns}) VALUES ({placeholders})"
    
    if tuples:
        total_rows = len(tuples)
        inserted = 0
        for i in range(0, total_rows, batch_size):
            batch = tuples[i:i + batch_size]
            cursor.executemany(sql_query, batch)
            inserted += len(batch)
            if total_rows > batch_size:
                print(f"  Inserted {inserted}/{total_rows} rows into movie_ratings_summary...", end='\r')
        if total_rows > batch_size:
            print()  # New line after progress indicator
        print(f"Data inserted successfully into movie_ratings_summary! ({total_rows} rows)")
    else:
        print("No data to insert into movie_ratings_summary")


def populate_all(cursor):
    """
    Populate all tables in the correct order to respect foreign key constraints.
    CSV files should be in the dataSets directory or in the parent directory.
    """
    print("Starting database population...")
    print("=" * 50)
    
    # Try to find CSV files in dataSets directory first, then parent directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    data_sets_dir = os.path.join(script_dir, "dataSets")
    parent_dir = os.path.dirname(script_dir)
    
    # Determine base path for CSV files
    if os.path.exists(os.path.join(data_sets_dir, "movies.csv")):
        base_path = "dataSets"
    elif os.path.exists(os.path.join(parent_dir, "movies.csv")):
        base_path = ".."
    else:
        base_path = "dataSets"  # Default to dataSets
        print(f"Warning: CSV files not found in expected locations. Using {base_path}/")
    
    # 1. Base tables (no foreign keys) - use INSERT IGNORE for duplicates
    print("\n1. Populating base tables...")
    populate_table_from_csv("movies", os.path.join(base_path, "movies.csv"), cursor, use_insert_ignore=True)
    populate_table_from_csv("genres", os.path.join(base_path, "genres.csv"), cursor, use_insert_ignore=True)
    populate_table_from_csv("people", os.path.join(base_path, "people.csv"), cursor, use_insert_ignore=True)
    populate_table_from_csv("keywords", os.path.join(base_path, "keywords.csv"), cursor, use_insert_ignore=True)
    
    # 2. Tables that reference base tables
    print("\n2. Populating relationship tables...")
    populate_table_from_csv("movie_genres", os.path.join(base_path, "movie_genres.csv"), cursor)
    populate_table_from_csv("movie_keywords", os.path.join(base_path, "movie_keywords.csv"), cursor)
    populate_movie_cast_table(os.path.join(base_path, "movie_cast.csv"), cursor)
    populate_movie_crew_table(os.path.join(base_path, "movie_crew.csv"), cursor)
    populate_movie_ratings_summary(os.path.join(base_path, "movie_ratings_summary.csv"), cursor)
    
    print("\n" + "=" * 50)
    print("Data successfully inserted into all tables!")


if __name__ == "__main__":
    try:
        # Connect to database using config
        connection = mysql.connector.connect(
            host=config.DB_CONFIG['host'],
            port=config.DB_CONFIG['port'],
            user=config.DB_CONFIG['user'],
            database=config.DB_CONFIG['database'],
            password=config.DB_CONFIG['password'],
        )
        cursor = connection.cursor()
        
        populate_all(cursor)
        connection.commit()
        print("\nAll changes committed successfully!")
        
    except mysql.connector.Error as err:
        print(f"\nDatabase error occurred: {err}")
        if 'connection' in locals():
            connection.rollback()
            print("Changes rolled back.")
    except Exception as e:
        print(f"\nError occurred: {e}")
        if 'connection' in locals():
            connection.rollback()
            print("Changes rolled back.")
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'connection' in locals():
            connection.close()

